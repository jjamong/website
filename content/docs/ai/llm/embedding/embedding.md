---
weight: 1
slug: index
date: 2025-06-10
title: "embedding(임베딩)"
description: "embedding(임베딩)"
toc: true
---

## 이해하기

임베딩은 사람이 쓰는 텍스트나 이미지 같은 데이터를  
**컴퓨터가 이해할 수 있는 숫자 벡터 형태로 바꾸는 과정**입니다.

예를 들어 “강아지”라는 단어를  `[0.23, -0.41, 0.91, ...]` 같은 벡터로 변환해서  
AI가 “고양이”와 비슷하다고 느낄 수 있게 만들어줍니다.

### 1. 비슷한 것 찾기
- 단어, 문장, 이미지 등을 임베딩하면
- 서로 의미가 가까운 것끼리 벡터 공간에서도 가깝게 위치합니다.
- 예: “사과”와 “배”는 가까이, “사과”와 “자동차”는 멀리 있음

### 2. 의미 기반 검색
- “서울 전망 좋은 레스토랑”처럼 자연어로 검색했을 때
- 단순 키워드 매칭이 아니라 **문장의 의미를 이해한 결과**를 보여줍니다.

### 3. 추천 시스템
- 사용자 행동을 임베딩해서 비슷한 취향의 사람이나 콘텐츠를 추천할 수 있습니다.

### 4. 감정 분석이나 분류
- 문장의 임베딩을 통해 긍정/부정 같은 감정을 파악하거나
- 글의 주제를 분류할 수 있습니다.

### 5. 멀티모달(텍스트+이미지) 처리
- 텍스트와 이미지를 같은 공간에 임베딩해서
- "고양이 사진"이라는 문장으로 실제 고양이 이미지를 찾을 수 있습니다.


## ⚙️ 임베딩은 어떻게 동작할까?

1. **데이터 입력**
   - 텍스트(단어, 문장), 이미지, 사용자 정보 등

2. **모델에 입력**
   - 사전 학습된 임베딩 모델에 데이터를 넣습니다

3. **벡터로 변환**
   - 모델은 입력 데이터를 고정된 차원의 숫자 벡터로 변환합니다
   - 예: `[0.12, -0.88, 1.03, ..., 0.25]` (512차원, 768차원 등)


## 🔍 임베딩 차원은 왜 중요할까?

임베딩 벡터는 보통 **512, 768, 1024차원** 등으로 구성됩니다.  
차원이 높을수록 표현력은 좋아지지만, 계산 비용과 저장 용량도 커집니다.

- **낮은 차원**: 빠르지만 정보 손실이 있을 수 있음
- **높은 차원**: 정확하지만 느리거나 무거움


## 🛠 대표적인 임베딩 모델 & 툴


| 이름 | 설명 | 활용 분야 |
|------|------|-----------|
| **Word2Vec** | 단어 임베딩의 고전, 단어 간 의미 파악에 탁월 | 단어 유사도, NLP 기초 |
| **GloVe** | 전역 통계 기반 단어 임베딩 | Word2Vec과 유사, 의미 파악 |
| **FastText** | 형태소 단위 임베딩으로 희귀 단어도 잘 처리 | 한국어 등 다양한 언어 |
| **BERT** | 문맥을 고려한 문장/문서 임베딩 | 자연어 이해, 질문응답 |
| **Sentence-BERT** | BERT 기반 문장 임베딩 모델 | 의미 기반 문장 검색, QA |
| **OpenAI Embedding API** | GPT 기반 문장/문서 임베딩 제공 | RAG, 검색, 유사도 매칭 |
| **CLIP (OpenAI)** | 텍스트-이미지 멀티모달 임베딩 | 이미지 검색, 생성 AI |
| **Instructor-XL** | 지시문 기반 임베딩, 특정 목적 최적화 | RAG 성능 향상용으로 인기 |
